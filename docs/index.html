<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <link rel="manifest" href="manifest.json" />
        <meta name="theme-color" content="#f5ebdc" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Set Game Detector</title>

        <style>
            body {
                font-family: sans-serif;
                margin: 0;
                padding: 0;
            }
            #container {
                display: flex;
                flex-direction: column;
                align-items: center;
                padding: 1rem;
            }
            video,
            canvas {
                max-width: 100%;
            }
            #status {
                margin-top: 1rem;
            }

            #perfBox {
                position: absolute;
                top: 10px;
                right: 10px;
                background: rgba(0, 0, 0, 0.6);
                color: white;
                padding: 6px 10px;
                border-radius: 6px;
                font-size: 14px;
                pointer-events: none;
                z-index: 5;
            }

            #nextBtn,
            #uploadBtn {
                margin-top: 1rem;
                padding: 8px 16px;
                font-size: 15px;
                position: relative;
                z-index: 10;
                display: none;
                cursor: pointer;
            }

            #uploadBtn {
                display: block;
            } /* always shown */

            #mainCanvas {
                position: relative;
                z-index: 1;
                display: none; /* hidden until a set is detected OR manual upload */
            }
        </style>
    </head>

    <body>
        <div id="container">
            <h2>Set Game Detection v1.4</h2>

            <video id="video" autoplay playsinline></video>

            <canvas id="mainCanvas" width="640" height="480"></canvas>

            <input type="file" id="uploadBtn" accept="image/*" />

            <button id="nextBtn">Next</button>

            <div id="status">Loading models...</div>

            <div
                id="cardList"
                style="
                    white-space: pre;
                    margin-top: 1rem;
                    font-family: monospace;
                    font-size: 14px;
                "
            ></div>

            <div id="perfBox">seg: -, clf: -, set: -</div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
        <script>
            "use strict";
            // =================================================
            // Global State
            // =================================================
            let tSeg = 0,
                tClf = 0,
                tSet = 0;
            let lastDetections = [];
            let lastFoundSet = null;
            let lastSets = [];
            let paused = false;
            let isProcessing = false;
            let capturedImage = null; // frozen frame as ImageData

            const COLORS = ["red", "green", "purple"];
            const SHAPES = ["diamond", "oval", "squiggle"];
            const FILLS = ["solid", "striped", "empty"];
            const COUNTS = ["one", "two", "three"];

            const YOLO_W = 640;
            const YOLO_H = 480;
            const IMG_SIZE = 128; // Classifier input size
            const CONF_THRESH = 0.7;
            const IOU_THRESH = 0.5;

            // Normalization constants
            const MEAN = [0.485, 0.456, 0.406];
            const STD = [0.229, 0.224, 0.225];

            const video = document.getElementById("video");
            const canvas = document.getElementById("mainCanvas");
            const ctx = canvas.getContext("2d", { willReadFrequently: true });

            const nextBtn = document.getElementById("nextBtn");
            const uploadBtn = document.getElementById("uploadBtn");
            const perfBox = document.getElementById("perfBox");

            /** @type {ort.InferenceSession | null} */
            let yoloSession = null;
            /** @type {ort.InferenceSession | null} */
            let clfSession = null;

            // OPTIMIZATION: Reusable canvas for resizing crops
            // 'willReadFrequently' speeds up getImageData significantly
            const batchCanvas = document.createElement("canvas");
            batchCanvas.width = IMG_SIZE;
            batchCanvas.height = IMG_SIZE;
            const batchCtx = batchCanvas.getContext("2d", {
                willReadFrequently: true,
            });

            // =================================================
            // Utilities
            // =================================================
            /**
             * @param {string|number} text
             * @param {number} x
             * @param {number} y
             */
            function drawCenteredText(text, x, y) {
                ctx.font = "30px sans-serif";
                ctx.fillStyle = "yellow";
                ctx.textAlign = "center";
                ctx.textBaseline = "middle";
                ctx.fillText(String(text), x, y);
            }

            /**
             * Helper to find argmax in a flat array at a specific offset
             */
            function argmaxOffset(data, offset, length) {
                let maxIdx = 0;
                let maxVal = data[offset];
                for (let i = 1; i < length; i++) {
                    if (data[offset + i] > maxVal) {
                        maxVal = data[offset + i];
                        maxIdx = i;
                    }
                }
                return maxIdx;
            }

            // =================================================
            // Preprocessors
            // =================================================

            // OPTIMIZATION: Pre-allocate float array for YOLO to avoid GC
            // (Assuming single image input always)
            const yoloInputData = new Float32Array(3 * YOLO_W * YOLO_H);

            function preprocessYOLO_fromImageData(img) {
                const d = img.data;
                // Reset pointers
                let r = 0,
                    g = YOLO_W * YOLO_H,
                    b = 2 * YOLO_W * YOLO_H;

                // Fill pre-allocated array
                for (let i = 0; i < d.length; i += 4) {
                    yoloInputData[r++] = d[i] / 255.0;
                    yoloInputData[g++] = d[i + 1] / 255.0;
                    yoloInputData[b++] = d[i + 2] / 255.0;
                }
                return new ort.Tensor("float32", yoloInputData, [
                    1,
                    3,
                    YOLO_H,
                    YOLO_W,
                ]);
            }

            /**
             * OPTIMIZATION: Batched Preprocessing
             * Takes all valid detection boxes, extracts crops, resizes,
             * normalizes, and creates ONE big tensor [N, 3, 128, 128].
             */
            function preprocessBatch(boxes, sourceCanvas) {
                const N = boxes.length;
                if (N === 0) return null;

                const stride = IMG_SIZE * IMG_SIZE;
                const channelStride = N * stride; // Not used for planar packing logic below, but useful concept

                // Flat Float32Array for [N, 3, H, W]
                const data = new Float32Array(N * 3 * stride);

                for (let i = 0; i < N; i++) {
                    const [x1, y1, x2, y2] = boxes[i];

                    // 1. Draw crop to shared small canvas (handles resizing)
                    // Note: sourceCanvas contains the full image.
                    const w = x2 - x1;
                    const h = y2 - y1;

                    // Clear usually not needed if we draw full rect, but safety first if aspect ratio issues
                    // batchCtx.clearRect(0, 0, IMG_SIZE, IMG_SIZE);

                    batchCtx.drawImage(
                        sourceCanvas,
                        x1,
                        y1,
                        w,
                        h,
                        0,
                        0,
                        IMG_SIZE,
                        IMG_SIZE,
                    );

                    // 2. Get Raw Pixels
                    const imgData = batchCtx.getImageData(
                        0,
                        0,
                        IMG_SIZE,
                        IMG_SIZE,
                    ).data;

                    // 3. Normalize and Pack into Batch Tensor
                    // Target layout: [Batch, Channel, Row, Col]
                    // R plane start for this image: i * (3 * stride)
                    // G plane start for this image: i * (3 * stride) + stride
                    // B plane start for this image: i * (3 * stride) + 2 * stride

                    let rStart = i * 3 * stride;
                    let gStart = rStart + stride;
                    let bStart = gStart + stride;

                    let pxIndex = 0;
                    for (let k = 0; k < imgData.length; k += 4) {
                        // R
                        data[rStart++] =
                            (imgData[k] / 255.0 - MEAN[0]) / STD[0];
                        // G
                        data[gStart++] =
                            (imgData[k + 1] / 255.0 - MEAN[1]) / STD[1];
                        // B
                        data[bStart++] =
                            (imgData[k + 2] / 255.0 - MEAN[2]) / STD[2];
                    }
                }

                return new ort.Tensor("float32", data, [
                    N,
                    3,
                    IMG_SIZE,
                    IMG_SIZE,
                ]);
            }

            // =================================================
            // YOLO decode
            // =================================================
            /**
             * @param {ort.Tensor} raw
             * @returns {Array<[number, number, number, number, number]>}
             */
            function decodeYOLO(raw) {
                const arr = raw.data;
                const stride = 6300;
                const cx = arr.subarray(0, stride);
                const cy = arr.subarray(stride, 2 * stride);
                const w = arr.subarray(2 * stride, 3 * stride);
                const h = arr.subarray(3 * stride, 4 * stride);
                const conf = arr.subarray(4 * stride, 5 * stride);

                const out = [];
                for (let i = 0; i < stride; i++) {
                    if (conf[i] < CONF_THRESH) continue;
                    const x1 = cx[i] - w[i] / 2;
                    const y1 = cy[i] - h[i] / 2;
                    const x2 = cx[i] + w[i] / 2;
                    const y2 = cy[i] + h[i] / 2;
                    if (x2 <= x1 || y2 <= y1) continue;
                    out.push([x1, y1, x2, y2, conf[i]]);
                }
                return out;
            }

            /**
             * @param {number[]} a
             * @param {number[]} b
             * @returns {number}
             */
            function iou(a, b) {
                const x1 = Math.max(a[0], b[0]),
                    y1 = Math.max(a[1], b[1]),
                    x2 = Math.min(a[2], b[2]),
                    y2 = Math.min(a[3], b[3]),
                    w = Math.max(0, x2 - x1),
                    h = Math.max(0, y2 - y1),
                    inter = w * h;
                if (!inter) return 0;
                const areaA = (a[2] - a[0]) * (a[3] - a[1]);
                const areaB = (b[2] - b[0]) * (b[3] - b[1]);
                return inter / (areaA + areaB - inter);
            }

            /**
             * @param {Array<[number, number, number, number, number]>} boxes
             * @returns {Array<[number, number, number, number, number]>}
             */
            function nms(boxes) {
                boxes.sort((a, b) => b[4] - a[4]);
                const keep = [];
                while (boxes.length) {
                    const a = boxes.shift();
                    keep.push(a);
                    boxes = boxes.filter((b) => iou(a, b) < IOU_THRESH);
                }
                return keep;
            }

            // =================================================
            // Logic: Sets
            // =================================================
            function attrOK(a, b, c) {
                return (a === b && b === c) || (a !== b && a !== c && b !== c);
            }
            function isSet(a, b, c) {
                return (
                    attrOK(a.color, b.color, c.color) &&
                    attrOK(a.shape, b.shape, c.shape) &&
                    attrOK(a.fill, b.fill, c.fill) &&
                    attrOK(a.count, b.count, c.count)
                );
            }
            /**
             * @param {{attrs:{color:string;shape:string;fill:string;count:string}}[]} cards
             * @returns {number[][]}
             */
            function findSets(cards) {
                const out = [];
                const n = cards.length;
                for (let i = 0; i < n; i++)
                    for (let j = i + 1; j < n; j++)
                        for (let k = j + 1; k < n; k++)
                            if (
                                isSet(
                                    cards[i].attrs,
                                    cards[j].attrs,
                                    cards[k].attrs,
                                )
                            )
                                out.push([i, j, k]);
                return out;
            }

            // =================================================
            // Main Loop
            // =================================================
            async function loop() {
                requestAnimationFrame(loop);
                if (paused) return;

                // Draw live video to canvas (needed for cropping later)
                ctx.drawImage(video, 0, 0, YOLO_W, YOLO_H);

                if (!isProcessing) {
                    // Grab image data once
                    capturedImage = ctx.getImageData(0, 0, YOLO_W, YOLO_H);
                    // fire and forget; do not await to keep UI responsive
                    void processFrame(capturedImage);
                }
            }

            // =================================================
            // Manual Upload
            // =================================================
            uploadBtn.onchange = async function (e) {
                const file = this.files && this.files[0];
                if (!file) {
                    return;
                }

                // Stop the loop BEFORE drawing, or video will overwrite the canvas
                paused = true;
                video.style.display = "none";
                canvas.style.display = "block";

                const img = new Image();
                img.onload = () => {
                    isProcessing = false;
                    capturedImage = null;
                    ctx.drawImage(img, 0, 0, YOLO_W, YOLO_H);
                    capturedImage = ctx.getImageData(0, 0, YOLO_W, YOLO_H);
                    processFrame(capturedImage);
                };
                img.src = URL.createObjectURL(file);
            };

            // =================================================
            // Process a single frame (stateless)
            // =================================================
            async function processFrame(imageData) {
                isProcessing = true;

                // 1. YOLO Segmentation
                // --------------------
                const t0 = performance.now();
                const yIn = preprocessYOLO_fromImageData(imageData);
                const yOut = await yoloSession.run({
                    [yoloSession.inputNames[0]]: yIn,
                });

                // Decode and NMS
                let dets = decodeYOLO(yOut[yoloSession.outputNames[0]]);
                dets = nms(dets);
                tSeg = performance.now() - t0;

                // Filter small/edge boxes
                const validDets = [];
                const crops = [];

                for (const box of dets) {
                    const [x1, y1, x2, y2, conf] = box;
                    // Clamp
                    const x1c = Math.max(0, Math.min(YOLO_W - 1, x1));
                    const y1c = Math.max(0, Math.min(YOLO_H - 1, y1));
                    const x2c = Math.max(0, Math.min(YOLO_W - 1, x2));
                    const y2c = Math.max(0, Math.min(YOLO_H - 1, y2));
                    const w = x2c - x1c;
                    const h = y2c - y1c;

                    if (w < 10 || h < 10) continue; // Skip tiny noise

                    validDets.push([x1c, y1c, x2c, y2c]);
                }

                // 2. Batched Classification
                // -------------------------
                const cards = [];
                const t1 = performance.now();

                if (validDets.length > 0) {
                    // Create ONE tensor containing all crops
                    // Pass 'canvas' because it holds the current image drawing
                    const batchTensor = preprocessBatch(validDets, canvas);

                    // Run inference once
                    const clfResults = await clfSession.run({
                        [clfSession.inputNames[0]]: batchTensor,
                    });

                    // Parse results
                    // The output tensors (color, shape, etc.) are now [N, 3] or [N, 4]
                    const N = validDets.length;
                    const colorData = clfResults.color.data;
                    const shapeData = clfResults.shape.data;
                    const fillData = clfResults.fill.data;
                    const countData = clfResults.count.data;

                    for (let i = 0; i < N; i++) {
                        // Determine offsets based on i * number_of_classes
                        // Assumes 3 classes per attribute based on global arrays
                        cards.push({
                            box: validDets[i],
                            attrs: {
                                color: COLORS[
                                    argmaxOffset(colorData, i * 3, 3)
                                ],
                                shape: SHAPES[
                                    argmaxOffset(shapeData, i * 3, 3)
                                ],
                                fill: FILLS[argmaxOffset(fillData, i * 3, 3)],
                                count: COUNTS[
                                    argmaxOffset(countData, i * 3, 3)
                                ],
                            },
                        });
                    }
                }
                tClf = performance.now() - t1;

                // 3. Set Logic
                // ------------
                const t2 = performance.now();
                const sets = findSets(cards);
                tSet = performance.now() - t2;

                lastDetections = cards;
                lastSets = sets;

                if (sets.length > 0) {
                    paused = true;
                    lastFoundSet = sets[0];
                    renderFrozenResults();
                    nextBtn.style.display = "block";
                }

                isProcessing = false;
            }

            function renderFrozenResults() {
                if (!capturedImage) return;
                video.style.display = "none";
                canvas.style.display = "block";

                ctx.putImageData(capturedImage, 0, 0);

                ctx.strokeStyle = "white";
                ctx.lineWidth = 2;
                lastDetections.forEach((c, i) => {
                    const [x1, y1, x2, y2] = c.box;
                    ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);
                    const cx = (x1 + x2) / 2;
                    const cy = (y1 + y2) / 2;
                    drawCenteredText(i + 1, cx, cy);
                });

                if (lastFoundSet) {
                    ctx.strokeStyle = "#00ff00";
                    ctx.lineWidth = 6;
                    lastFoundSet.forEach((idx) => {
                        const [x1, y1, x2, y2] = lastDetections[idx].box;
                        ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);
                    });
                }

                perfBox.textContent =
                    `seg: ${tSeg.toFixed(1)}ms, ` +
                    `clf: ${tClf.toFixed(1)}ms (n=${lastDetections.length}), ` +
                    `set: ${tSet.toFixed(1)}ms`;

                const cardLines = lastDetections.map((c, i) => {
                    const a = c.attrs;
                    return `Card ${i + 1}: ${a.count} ${a.color} ${a.shape} ${a.fill}`;
                });
                const setLines = lastSets.map(
                    (s, i) => `Set ${i + 1}: ${s.map((x) => x + 1).join(", ")}`,
                );

                document.getElementById("cardList").textContent = [
                    ...cardLines,
                    ...setLines,
                ].join("\n");
            }

            nextBtn.onclick = () => {
                paused = false;
                nextBtn.style.display = "none";
                video.style.display = "block";
                canvas.style.display = "none";
            };

            async function init() {
                if (!("mediaDevices" in navigator)) {
                    document.getElementById("status").textContent =
                        "Camera API not available.";
                    return;
                }

                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: { ideal: "environment" },
                        width: { ideal: YOLO_W },
                        height: { ideal: YOLO_H },
                    },
                });
                video.srcObject = stream;

                // NOTE: For batched classification to work,
                // the ONNX model must be exported with dynamic axes
                // on the first dimension (batch size).
                try {
                    yoloSession = await ort.InferenceSession.create(
                        "segmentationv3.onnx",
                        {
                            executionProviders: ["webgpu"],
                            graphOptimizationLevel: "all",
                        },
                    );
                    clfSession = await ort.InferenceSession.create(
                        "classificationv3.onnx",
                        {
                            executionProviders: ["webgpu"],
                            graphOptimizationLevel: "all",
                        },
                    );
                    document.getElementById("status").textContent =
                        "Models loaded (WebGPU Optimized).";
                    requestAnimationFrame(loop);
                } catch (e) {
                    console.error(e);
                    document.getElementById("status").textContent =
                        "Error loading models: " + e;
                }
            }

            init();
        </script>

        <script>
            "use strict";

            if ("serviceWorker" in navigator) {
                navigator.serviceWorker
                    .register("service-worker.js")
                    .catch((e) => console.error("SW registration failed", e));
            }
        </script>
    </body>
</html>
